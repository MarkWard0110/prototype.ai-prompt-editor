@page "/"
@using BAIsic.LlmApi.Ollama
@using PromptEditor
@inject IJSRuntime JS
@inject IModelInvoker ModelInvokerService
@inject IOllamaService OllamaService

<PageTitle>AI Prompt Editor</PageTitle>

<div id="appJs">
    <div id="side-bar" class="split">
        <div id="prompt-commands">
            <button id="flag-prompt-btn">Flag Prompt</button>
            <button id="invoke-btn">Invoke</button>
        </div>
        <label for="revision-history">History:</label>
        <div id="revision-history">
            <!-- Revision history tree will be dynamically populated here -->
        </div>

        <div id="variables-section">
            <h3>Variables</h3>
            <div id="variables-inputs">
                <input type="text" id="variable-name" placeholder="Variable name" />
                <textarea id="variable-value" placeholder="Value"></textarea>
            </div>
            <button id="add-variable-btn">Add Variable</button>
            <ul id="variables-list">
                <!-- Variables list will be populated here -->
            </ul>
        </div>
        <div><button id="extract-flagged-btn">Extract Flagged Nodes</button></div>
        <div id="advanced-area">
            <button id="toggle-advanced">Show Advanced Options</button>
            <div id="advanced-prompt-options" style="display: none;">
                <div>
                    <label for="stop-word-list">Stop Words:</label>
                    <input type="text" id="stop-word-list" placeholder="(default)" />
                </div>
                <div>
                    <label for"num_ctx">Context Size:</label>
                    <input type="number" id="num_ctx" name="num_ctx" min="1" value="2048">
                </div>
                <div>
                    <label for="num_predict">Predict Size:</label>
                    <!-- Maximum number of tokens to predict when generating text. (Default: 128, -1 = infinite generation, -2 = fill context) -->
                    <input type="number" id="num_predict" name="num_predict" min="-2" value="-1">
                </div>
                <div>
                    <button id="load-chat-btn">Load Chat</button>
                    <textarea id="load-chat-value" placeholder=""></textarea>
                </div>
            </div>
        </div>
        <div><button id="new-session-btn">New Session</button></div>
    </div>
    <div class="gutter gutter-vertical"></div>
    <div id="prompt" class="split">
        <div id="prompt-options">
            <label for="ai-model">AI Model:</label>
            <select id="ai-model">
            </select>
            <label for="temperature">Temperature:</label>
            <input type="number" id="temperature" name="temperature" min="0.0" max="1.0" step="0.01" value="0.0">
            <label for="top_p">Top P:</label>
            <input type="number" id="top_p" name="top_p" min="0.0" max="1.0" step="0.01" value="0.0">
        </div>
        <div id="chat-prompts">
        </div>
        <button id="add-message-btn">Add Message</button>
    </div>
    <div class="gutter gutter-vertical"></div>
    <div id="response" class="split">
        <label for="invoke-response">Response:</label>
        <div>
            <textarea id="invoke-response" placeholder="response..."></textarea>
        </div>
    </div>
</div>

@code {

    private DotNetObjectReference<Home> _dotNetObjectReference;

    protected override void OnInitialized()
    {
        _dotNetObjectReference = DotNetObjectReference.Create(this);
        JS.InvokeVoidAsync("setDotNetObjectRef", _dotNetObjectReference);
    }


    protected override async Task OnAfterRenderAsync(bool firstRender)
    {
        if (firstRender)
        {
            await JS.InvokeVoidAsync("initApp");
        }
    }

    [JSInvokable]
    public async Task<ChatResponse> InvokeModelAsync(InvokeRequest request)
    {
        try
        {
            var result = await ModelInvokerService.InvokeModelAsync(request);
            return result;
        }
        catch (Exception ex)
        {
            Console.WriteLine(ex.Message);
        }
        return new ChatResponse()
            {
                Message = new Message()
                {
                    Role = "AI",
                    Content = "An error occurred while invoking the model."
                }
            };
    }

    [JSInvokable]
    public async Task<string[]> ListModelsAsync()
    {
        try
        {
            var models = await OllamaService.ListModelsAsync();
            return models;
        }
        catch (Exception ex)
        {
            Console.WriteLine(ex.Message);
            return new string[] { };
        }
    }

}